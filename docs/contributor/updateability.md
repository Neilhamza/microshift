# Upgrading MicroShift

Following document is meant to be a contributor's introduction
to the feature of MicroShift's updateability.

## On-disk artifacts

New on-disk artifacts were added:
- `/var/lib/microshift-backups/`
    - `health.json`
    - backup directories
- `/var/lib/microshift/version`

### `/var/lib/microshift-backups/`

New, additional directory for keeping health and backup related artifacts.
For ostree systems it's a non-configurable directory where backups are created.
For non-ostree systems it's a default (configurable) directory for backups.

#### `health.json` [ostree-only]

Contains information about health of the system.
It's updated by green and red script supplied by MicroShift.

Because MicroShift's healthcheck takes some time to assess the state of the MicroShift,
this file might be updated after couple minutes into system boot.

Schema:
```json
{
    "health": "",
    "deployment_id": "",
    "boot_id": ""
}
```

- `health`: System status according to the greenboot healthchecks.
  - Expected values: `healthy` or `unhealthy`
- `deployment_id`: OSTree deployment ID.
  - Obtained with a command: `rpm-ostree status --json | jq '.deployments[].id'`
- `boot_id`: Boot ID generated by kernel.
  - Obtained with command: `tr -d '-' < /proc/sys/kernel/random/boot_id`.
  - Hyphens are removed to match format used by `journalctl` (see `journalctl --list-boots --reverse`)


#### Backup directories

> Note: Only ostree-based systems perform automated backups and restores.

`/var/lib/microshift-backups/` also stores backups of MicroShift data.
Backups are created by copying data directory with `--reflink`
option to leverage Copy-on-Write functionality.

Naming schema of backup directories: `deployment-id_boot-id[_unhealthy]`.

Suffix `_unhealthy` is used to mark backups of unhealthy systems.
These backups are performed when all existing metadata suggests that it is expected
that MicroShift should wipe the data and start from clean state
(for example "FDO" scenario where unhealthy deployment leaves stale data).

### `/var/lib/microshift/version`

This file holds version of MicroShift that ran or attempted to run
using the data.

On first start (defined at non-existence of the data), MicroShift writes
its version to the file.

On following starts, MicroShift will compare version stored in the file
with version of the executable to check if an upgrade is allowed or should be blocked
(see [Version metadata management](#version-metadata-management) for more information).

Schema of `version` file:
```
MAJOR.MINOR.PATCH
```

All of MAJOR, MINOR, and PATCH are unsigned integers. For example: `4.14.0`

> Note: MicroShift writes to a file without trailing newline.
> However, it should successfully handle a file with newline.

## Greenboot

[Greenboot](https://github.com/fedora-iot/greenboot) is a health check framework,
primarily for ostree-based systems.

For more in-depth information about greenboot and how MicroShift integrates
with it see following enhancements:
[Integrating MicroShift with Greenboot](https://github.com/openshift/enhancements/blob/master/enhancements/microshift/microshift-greenboot.md)
[MicroShift updateability in ostree based systems: integration with greenboot](https://github.com/openshift/enhancements/blob/master/enhancements/microshift/microshift-updateability-ostree.md#integration-with-greenboot)

In short, it works by running health check scripts
(placed in dirs: `/etc/greenboot/check/{required.d,wanted.d}/`) and, depending
on result of *required* scripts, will run either "green" (healthy) or
"red" (unhealthy) scripts. 

Greenboot heavily integrates with grub. In fact, it is the grub that decrements
the `boot_counter` and actually rolls back the system (by selecting the other
boot target). See [Grub](#grub-ostree) section for more information.

MicroShift delivers these scripts in `microshift-greenboot` RPM package.

Health check script for MicroShift is resides in the repository
as `packaging/greenboot/microshift-running-check.sh`.

Pre-updateability (and continues to do so), `microshift-greenboot` started to
provide `packaging/greenboot/microshift-pre-rollback.sh` file as a "red" script.
If `boot_counter` equals `0`, the script will run `microshift-cleanup-data --ovn`.

> Note: It's a red script, so it runs after healthchecks decided that
> system is unhealthy, and cleanup only happens when `boot_counter`
> already fell down to `0`.

### Updateability additions to `microshift-greenboot`

Updateability feature added two, new files - one "green" and one "red" script:
- `packaging/greenboot/microshift_set_healthy.sh` -> `/etc/greenboot/green.d/40_microshift_set_healthy.sh`
- `packaging/greenboot/microshift_set_unhealthy.sh` -> `/etc/greenboot/red.d/40_microshift_set_unhealthy.sh`

They work only on ostree-based systems (by checking if `/run/ostree-booted` exists).
Reason for this is two-fold: how greenboot works on non-ostree systems, and
deployment ID not being available, leaving only boot ID and health, which
are not as useful without the deployment as backup name.
Also, since automated backup management is only intended for ostree systems,
there is no gain in creating `health.json` (also: information about the health
can be found in the journal with greater context).

Primary responsibility of "set healthy" and "set unhealthy" scripts
is to create or update `health.json`.

`microshift_set_unhealthy.sh` has an additional logic that, before
updating `health.json`, checks if backup matching deployment and boot IDs
from the `health.json` exists, and does not update the file if not.
This ensures that, when upgrading (from a healthy system) MicroShift fails to
create a backup, in effect causing the system to be unhealthy, that the new health
information does not overwrite existing one ("healthy" instructs MicroShift to
create a backup).
Without this mechanism in place, healthy data of previous deployment, would not
get backed up, `health.json` get updated with `unhealthy` causing subsequent
starts of MicroShift want to restore a backup (which might not exist because it
wasn't created, or it's outdated because it contains data of
"second to last boot of previous deployment").

### Greenboot on non-ostree systems

Although greenboot can be installed and used on non-ostree system, its
usefulness is greatly diminished. It does not perform automated reboots that
try to help system get healthy, nor does it cause a rollback to previously
working system or MicroShift version.

## (rpm-)ostree [ostree-only]

TODO

### `rpm-ostree` vs `ostree`

TODO

### Commits and deployments

TODO

<!--
ostree     -> commits
rpm-ostree -> deployments

Deployment is a commit with a boot entry.
-->

## Useful `ostree` commands

TODO

## Useful `rpm-ostree` commands

TODO

## Grub [ostree]

While greenboot sets and clears env vars such as `boot_counter` and `boot_success`,
the data they store is actually acted upon by the grub.
Package `grub2-tools` provides several files directly involved in that process.

#### 08_fallback_counting

File resides in `/etc/grub.d`.

This file contains logic to do the actual counting down of the `boot_counter`.
Functionality is only executed if `boot_counter` exists and `boot_success` equals `0`.

> This is important because there is a `grub-boot-success.{timer,service}`
> which, after 2 minutes from user's logon, sets `boot_success` to `1`
> and thus causing `boot_counter` to not be decremented.

If `boot_counter` is `0` or `-1`, then variable `default` is set to `1`.
Otherwise, `boot_counter` is decremented
(*it's set by the greenboot to a configurable value, by default `3`, when staging new deployment*).

Setting `default` to `1` effectively changes which boot entry (1st one) will boot.

#### 10_reset_boot_success and 12_menu_auto_hide

Files reside in `/etc/grub.d`.

`10_reset_boot_success` contains logic for deciding to hide the boot menu
depending on values of `boot_success` (*last boot was ok*) and `boot_indeterminate` 
(*first boot attempt to boot the entry*) and may change values of these to
variables (and `menu_hide_ok`).

`12_menu_auto_hide` makes use of `menu_hide_ok` variable.

These scripts were not yet observed to have a big impact on efforts related to
updateability feature.

### grub-boot-success.{timer,service}

Both files are installed into `/usr/lib/systemd/user/` meaning they are intended
for a user, rather than a system.

`grub-boot-success.timer` is a 2 minute timer that starts when user logs in.
When user session is active for 2 minutes, it triggers `grub-boot-success.server`
which sets `boot_success`.

> **IMPORTANT**: Above means that active user session influences whole
> ostree+greenboot+grub integration. By setting `boot_success` to `1` it causes
> grub to not decrement the `boot_counter`.
>
> [Discussion on fedora-iot/greenboot about this particular issue](https://github.com/fedora-iot/greenboot/issues/108).

### Other files

There are other services that are involved in grub's envvar management: 
- `/usr/lib/systemd/system/grub-boot-indeterminate.service`
- `/usr/lib/systemd/system/grub2-systemd-integration.service`

Their impact on MicroShift was not investigated and is currently unknown,
however it is not expected to be significant.

## "grub * greenboot * rpm-ostree" integration summary

<!-- 
TODO
Staging deployment causes greenboot to set boot_counter
boot_counter is decremented on each boot by the grub
if falls down to 0 or is already -1, it's set to -1, and default boot entry is changed to `1`

greenboot starts, sees -1, issues `rpm-ostree rollback` which from currently booted systems is kind of a no-op, because
the rollback deployment is already booted, but this ensures that rebooting the system will result in the same.
-->

## MicroShift Updateability Implementation

Upgradeability introduces following features that run in following order:
- backup management,
- version metadata management,
- Kubernetes storage migration.

These three areas are mostly separate, but backup management
may inspect version metadata to handle some corner cases.
Backup management and version metadata management run
before MicroShift attempts to start all the components and cluster itself.
Storage migration runs when cluster is up and running 
(and in 10 minutes intervals afterwards) because it
requires runtime components to correctly perform its task.

### Automated backup management [ostree-only]

It runs very close to the start of MicroShift's `run` command.

It checks for existence of `/run/ostree-booted` file.
If the file is not present, it is assumed that the system is regular RPM based
and this stage is skipped without an error.

#### Handling unusual scenarios

Then, procedure checks existence of three on-disk artifacts:
- `/var/lib/microshift` (ignores `.nodename`[0]) - referred to as data below
- `/var/lib/microshift/version`
- `/var/lib/microshift-backups/health.json`

[0] `.nodename` might be created by the "effective config generation" phase
which happens before the prerun. This can create `.nodename` and result
in `/var/lib/microshift` being present but, beside that one file, empty.

##### `/var/lib/microshift` (data) does not exist

> Lack of `/var/lib/microshift` implies that `/var/lib/microshift/version` does not exist.

If both data and `health.json` are missing, then we assume it's a first run of MicroShift.

If data does not exist but `health.json` is present, it means that MicroShift ran on the system already.
Perhaps it was unhealthy and admin decided to delete the data dir to try clean start.
In such case MicroShift will inspect `health.json` and depending of value of `health`:
- `healthy` - last boot with MicroShift was OK, but there is no data?
  There's nothing to back up, so MicroShift will continue start up.
- `unhealthy` - MicroShift will look for a healthy backup for currently
  booted deployment 
  - if backup is found, it'll be restored
  - if there is no backup to restore from, MicroShift will continue start up (no data = clean start).


##### Data exists, but `version` does not

If `health.json` exists, then what could have happened is failed upgrade from 4.13,
resulting in system rollback, followed by admin's manual intervention by removing
`/var/lib/microshift`, but keeping `/var/lib/microshift-backups`.

That's why, ignoring completely existence of `health.json`,
if `/var/lib/microshift` exists but `version` file does not,
MicroShift will treat the data as an upgrade from MicroShift 4.13 
(because it's the last release not featuring that file), create a backup
named `4.13` and proceed with the start up.

> TODO: if system rolls back to 4.13 deployment, and an upgrade is attempted again
> without manually deleting `4.13` backup after manually restoring it,
> MicroShift will fail to make a backup and exit with an error
> (because backup with such name already exists).
> It feels like we should handle this more gracefully.

##### Both data and `version` exist, but `health.json` does not

On the very first run of a deployment featuring MicroShift,
none of the `/var/lib/microshift` or `/var/lib/microshift-backups` exist.
In such case `health.json` will exists only after greenboot-healthcheck finishes
the assessment and executes green or red scripts.

Possible, common scenarios that can get MicroShift into that state:
- Restart of microshift.service to reload the config
- Power loss or unexpected reboot of the machine before end of greenboot-healthcheck

To gracefully handle such scenarios, MicroShift will skip backup management
and proceed with the start up.

> Side note on non-first boots where `health.json` exists,
> but actions derived from its data were already performed:
>
> In case of microshift.service restart (e.g. to reload the config),
> it will re-attempt to perform the same actions it did on first start, for example:
> - `healthy`: back up the data - which won't happen, because backup already exists (not an error) 
> - `unhealthy`: attempt an "unhealthy system procedure" (described later).


#### Regular, expected in most cases, backup management procedure

##### *Boot ID stored in `health.json` matches current boot's ID: skip backup management and continue with start up

It means that information in `health.json` is intended for next boot
(be acted upon after system reboot) - it is a consequence of decision to
perform backup management on system boot.


##### `health.json` contains `healthy`

First, MicroShift will attempt to create a backup of the data named
with deployment and boot IDs from the `health.json`.

Then, if deployment ID stored in `health.json` differs from currently 
booted deployment's ID, it means different deployment was booted.

This can happen when:
- system is upgraded to a new deployment,
- system was rolled back,
- admin manually booted rollback deployment in grub menu.

To handle "going back to rollback deployment" scenario, MicroShift will check
the list of existing backups for one suitable for the currently booted deployment,
and, if found, it will restore that backup.
If no backup is found (e.g. upgrade), then MicroShift will continue start up.


##### `health.json` contains `unhealthy`

> Following steps might not be in the same order as implementation
> but should be functionally the same.

Summary:
- Try to restore backup for current deployment and continue start up
- If rollback deployment doesn't exist: continue start up
- Try to restore backup for rollback deployment and continue start up
- If there is no backup for neither the current nor rollback deployments:
  remove the data and continue start up (fresh)
- If `health.json` contains rollback deployment ID: exit with failure
- If `deployment_id` in `health.json` is neither current nor rollback deployments' ID:
  make "unhealthy" backup, remove the data and continue start up (fresh).

**If a backup for current deployment exists**: MicroShift will try to restore it and continue start up.

**If such backup does not exist**: MicroShift will try to restore backup for the rollback deployment.

**If rollback deployment does not exist** (i.e. there's only one deployment on the system):
it might be that system was manually rebooted or microshift.service manually restarted.
(It's not a reboot initiated by the greenboot because it only happens when `boot_counter` is set
and it's only set when deployment is staged.)
MicroShift will skip the backup management and continue with start up.

**If rollback deployment matches deployment ID persisted in the `health.json`**:
it means that it's an attempt to upgrade from unhealthy system which is unsupported -
if admin wants to perform such action, they should either get system to a healthy state,
or (if the MicroShift is the culprit) clear MicroShift data to start new
(and also get system to a healthy state).
In such case MicroShift will exit with an error that should render system unhealthy and eventually roll back.

> Extra comment:
>
> I think that on 2nd boot we'll lose that information resulting in different route:
> `health.json`.deployment_id == current deployment, health is `unhealthy`. 
> no backup to restore (neither current nor rollback deployment),
> so MicroShift will delete the data to start fresh.
> I think this might be confusing and unexpected (especially old data will be deleted).
> See [Test idea: upgrading from unhealthy system is blocked](#test-idea-upgrading-from-unhealthy-system-is-blocked)

**If deployment ID in `health.json` matches current deployment and backup for rollback deployment exists**:
restore and continue start up.
When upgrading the system fails during first boot of new deployement and renders system unhealthy,
on subsequent boots there is no backup for new deployment that could be restored.
Also, during subsequent boots MicroShift will consider the data unhealthy
(because that's the status in the `health.json`).

To gracefully handle such scenario, MicroShift will try to restore a backup
of the rollback deployment. This means that subsequent boots of new deployment
will have the same starting point as the first boot
(data belongs to previous deployment and is healthy).

> Extra comment:
> 
> This behaviour was previously designed so in case of failure to migrate the storage.
> It gives another chance to perform the migration starting from the initial state
> (that is starting with the data of previous deployment;
> starting just like it's first boot of new deployment).
>
> Since migration happens when full cluster is up and running,
> we might need to verify if this approach is still valid:
> - Does failure to migrate some CR will affect MicroShift's and in consequence system's health?

**If there is no backup for the rollback deployment**: remove the data and start fresh.
No backup of MicroShift data for rollback deployment might happen when rollback deployment
didn't feature MicroShift at all.
There is no backup to restore and current deployment is unhealthy -
we are not sure if MicroShift is the culprit, but being so early into
"MicroShift's lifetime" on the host we are not risking much when removing the data.
It's also a symmetrical with "restoring rollback deployment's backup"
where the data is restored so subsequent boots would start from the same point as first boot.

**If deployment ID in `health.json` doesn't match neither the rollback nor currently booted deployment**:
make an unhealthy backup and remove the data to allow for fresh start.


#### More details on backing up and restoring the data

Unhealthy backups were mentioned couple of times.
It must be noted that, when restoring the data, they are not considered a
candidates for the restore.

MicroShift will try its best to only keep one backup for particular deployment.
It means that before backing up, a list of backup for the deployment is obtained
and, after backing up, the list is used for deleting
"prior existing backups for the deployment that we
no longer need since we just backed up most up to date data".
This includes unhealthy backups.

If a particular backup (i.e. `deploymentID_bootID` combination),
there will be no new attempt to create a backup with that name again.
This is not considered an error - MicroShift will continue start up.

After successfully backing up the data, MicroShift compares list of
existing backups with deployments present on the system and removes any backup
which deployment is no longer present on the system.

### Version metadata management

This stage is all about comparing executable's and data's versions
to allow or block an upgrade and, if allowed, updating data version.

#### Version of the executable (`microshift` binary)

Version of executable is obtained from values embedded during built time:
```
# Makefile
 -X github.com/openshift/microshift/pkg/version.majorFromGit=$(MAJOR) \
 -X github.com/openshift/microshift/pkg/version.minorFromGit=$(MINOR) \
 -X github.com/openshift/microshift/pkg/version.patchFromGit=$(PATCH) \
```

> Hint:
>
> Values in Makefile can be overridden which was used to create
> "fake-next-minor" RPMs and commit - which is current code with
> newer minor version used in some tests to verify that version
> management works as expected.

#### Version of the data

Version of the data is loaded from `/var/lib/microshift/version` file.
If data exists, but the file does not, MicroShift assumes version
of the data is 4.13.

If real (from file) or assumed (4.13.0) data version is known, it will be
compared against version of the executable.
If data does not exist, MicroShift skips the checks and
proceeds to creation of the file.

#### Version compatibility

Following procedures compares the two versions making sure that:
- major versions are the same
- version of the data is not newer than version of the executable (e.g. downgrade)
- if executable is newer, then it's only by one minor version

#### Blocking certain upgrade paths

Executable and data versions are compared against a list of "blocked upgrades".
The list does not exist yet, but it is expected to be placed in 
`assets/release/upgrade-blocks.json` file with following schema:

```json
{
  "to-binary-version": ["blocked", "from", "data", "versions"],
  "to-binary-version-2": ["blocked", "from", "data", "versions"]
}
```
For example:
```json
{
		"4.14.10": ["4.14.5", "4.14.6"],
		"4.15.5":  ["4.15.2"]
}
```

Mechanism searches for the "top-level" version that matches executable's version
and, if one is found, checks if version of the data is present in the associated
list of version from which upgrade is blocked.
For example (using json data from above), if executable's version if "4.14.10"
and `version` file contains "4.14.5", then MicroShift will refuse to run with 
an error: "upgrade from '4.14.5' to '4.14.10' is blocked".

#### Updating `/var/lib/microshift/version`

If the data does not exist (i.e. it is a first run of MicroShift), then
file is created with version of the executable.

If file existed and version checks were successful, the file is updated 
with version of the executable.

### Kubernetes Storage Migration

Storage migration is process of updating objects to newer versions,
for example from `v1beta1` to `v1beta2`.

Initially, the plan was to perform the migration with only etcd and 
kube-apiserver running.
This turned out to not be a viable options because migrating
CustomResourceDefinitions might require webhooks which run as a Pods, 
therefore requiring kubelet, CNI, DNS, scheduler, and other components.

Right now, storage migration is comprised of two additional controllers
embedded into the MicroShift: Kube Storage Version Trigger and
Kube Storage Version Migrator. They make use of the `StorageState`
and `StorageVersionMigration` CR.

### `StorageState` and `StorageVersionMigration` Custom Resources

It is a Custom Resources created by the Trigger and
picked up to handle by the Migrator.
It can be also used to monitor progress of the migration and get
information about any failures.

#### Kube Storage Version Trigger

Trigger controller performs the discovery and creates a `StorageState` and
a `StorageVersionMigration` respectively.
The `StorageState` contains the current discovered state of the API resource
and the previous, if there is a version difference for that specific API resource,
then a `StorageVersionMigration` is created by the Trigger.
This check happens on start and every 10 minutes (discovery period;
hardcoded in upstream code).

> Hint: Restarting microshift.service may be used a way to quickly
> retrigger the migration without waiting 10 minutes.

#### Kube Storage Version Migrator

Migrator watches for `StorageVersionMigration` CRs and for each
will attempt to migrate related Kubernetes object.

The very core of the Migrator's implementation is simply: `GET` resource from
the API Server and `PUT` it back.
(Almost) all the heavy lifting is done by the API Server.

If schema of a CustomResourceDefinition changed substantially and requires custom logic
to perform the conversion between the versions, a conversion webhook needs to be provided.
If there are no schema changes between versions, API Server should handle
the conversion by itself.

For more information on migrating CRDs see 
[Versions in CustomResourceDefinitions](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/)
from Kubernetes documentation.

### Manual backup management (non-ostree)

Automated backup management is intended only for ostree-based systems.
For other, "regular RPM-based", systems `microshift` binary exposes following
commands aiming to help with creating and restoring backups of the MicroShift data:
- `microshift admin data backup`
- `microshift admin data restore`

> TODO: Initially `microshift admin` was supposed to group more subcommands.
> Maybe it's worth dropping at least one level? E.g. `microshift [admin|data} {backup,restore}`

##### `microshift admin data backup`

The command creates a backup of current MicroShift data (`/var/lib/microshift`).
It requires that `microshift.service` has status `exited` which means that
MicroShift must not be running (to avoid corrupting data if etcd was running
during the process and modifying the files).

Currently, if `microshift.service`'s status is `failed`, the command will
refuse to proceed.
This is based on assumption that `failed` means the MicroShift stopped running 
due to some runtime error and systemd gave up on restarting the service.
This suggests that MicroShift's data might not be healthy and thus should not
be backed up (mere existence of a backup suggests it contains valid data,
just like it is on ostree-based system with some exceptions that are explicitly
marked).

Command has two options:
- `--name` - backup name (effectively name of the directory which will hold the copy of the data).
  Default value contains MicroShift's version and current datetime: `MAJOR.MINOR__YYYYmmDD_HHMMSS`
- `--storage` - which a parent directory that will hold the backup.
  Default value is the same as for ostree-based systems: `/var/lib/microshift-backups`

> **TODO: Change default --name to contain version from `/var/lib/microshift/version` rather than executable's version**

> **TODO: Change default --name to include PATCH version**


##### `microshift admin data restore`

> **TODO: This command is not implemented yet.**

> **TODO: Decide: reuse `--name` and `--storage` from `backup` subcommand, or expect a full path?**

# Test ideas

## Test idea: upgrading from unhealthy system is blocked

- System should roll back to "original" unhealthy deployment
- Greenboot should declare "system need manual intervention"

## Test idea: upgrade blocking by producing RPMs with fake versions.